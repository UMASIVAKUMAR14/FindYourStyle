{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti0DEighHP2C"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "from typing import List\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw0pgImWd9sZ",
        "outputId": "6aba7361-b377-4c0c-9d38-a11849a12d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI-0MEk5IgVQ"
      },
      "outputs": [],
      "source": [
        "IMAGE_DIR = \"../data/images/\"\n",
        "SEGM_DIR = \"../data/segm/\"\n",
        "FABRIC_ANN_PATH = \"../data/labels/shape/texture\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfs-IJzMHLTw",
        "outputId": "928dc5bd-f8f8-4887-dc46-9f5209c0c915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data\t\t\timages.zip\t\t model\t    view\n",
            "DeepFashion-MultiModal\timages.zip4thj6ar8.part  README.md\n",
            "FindYourStyle\t\tlabels.zip\t\t segm.zip\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "69e15864"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/FindYourStyle/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcEX8KbFIoXR",
        "outputId": "6dcbbf0b-56ef-433f-a496-ffe61ca2c37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded fabric annotations: 44096\n"
          ]
        }
      ],
      "source": [
        "fabric_ann = {}\n",
        "\n",
        "with open(FABRIC_ANN_PATH, \"r\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        img_name = parts[0]\n",
        "        fabrics = list(map(int, parts[1:]))  # [upper, lower, outer]\n",
        "        fabric_ann[img_name] = fabrics\n",
        "\n",
        "print(\"Loaded fabric annotations:\", len(fabric_ann))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amLIFjVT0pEN"
      },
      "outputs": [],
      "source": [
        "def safe_ce_loss(logits, targets, criterion, ignore_index=7):\n",
        "    \"\"\"\n",
        "    Computes CrossEntropyLoss only if there is at least one valid target\n",
        "    (i.e., not ignore_index). Returns None otherwise.\n",
        "    \"\"\"\n",
        "    valid = (targets != ignore_index)\n",
        "    if valid.sum().item() == 0:\n",
        "        return None\n",
        "    return criterion(logits[valid], targets[valid])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1f8ed45"
      },
      "outputs": [],
      "source": [
        "class DeepFashionMultiFabricDataset(Dataset):\n",
        "    def __init__(self, img_dir, segm_dir, fabric_ann, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.segm_dir = segm_dir\n",
        "        self.fabric_ann = fabric_ann\n",
        "        self.transform = transform\n",
        "\n",
        "        segm_bases = set(\n",
        "            f.replace(\"_segm.png\", \"\") for f in os.listdir(segm_dir)\n",
        "            if f.endswith(\"_segm.png\")\n",
        "        )\n",
        "\n",
        "        self.files = [\n",
        "            img_name for img_name in fabric_ann.keys()\n",
        "            if img_name.replace(\".jpg\", \"\") in segm_bases\n",
        "        ]\n",
        "\n",
        "        print(f\"Dataset size: {len(self.files)}\")\n",
        "\n",
        "        # Define segmentation labels per garment\n",
        "        self.UPPER_LABELS = {1, 4, 21}\n",
        "        self.LOWER_LABELS = {5, 6}\n",
        "        self.OUTER_LABELS = {2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def _make_mask(self, segm_np, labels):\n",
        "        mask = np.isin(segm_np, list(labels)).astype(np.float32)\n",
        "        mask = Image.fromarray(mask).resize((224, 224), Image.NEAREST)\n",
        "        return torch.tensor(np.array(mask)).unsqueeze(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.files[idx]\n",
        "\n",
        "        image = Image.open(os.path.join(self.img_dir, img_name)).convert(\"RGB\")\n",
        "\n",
        "        segm_name = img_name.replace(\".jpg\", \"_segm.png\")\n",
        "        segm = Image.open(os.path.join(self.segm_dir, segm_name))\n",
        "        segm_np = np.array(segm)\n",
        "\n",
        "        upper_mask = self._make_mask(segm_np, self.UPPER_LABELS)\n",
        "        lower_mask = self._make_mask(segm_np, self.LOWER_LABELS)\n",
        "        outer_mask = self._make_mask(segm_np, self.OUTER_LABELS)\n",
        "\n",
        "        upper_label = torch.tensor(self.fabric_ann[img_name][0], dtype=torch.long)\n",
        "        lower_label = torch.tensor(self.fabric_ann[img_name][1], dtype=torch.long)\n",
        "        outer_label = torch.tensor(self.fabric_ann[img_name][2], dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (\n",
        "            image,\n",
        "            upper_mask, upper_label,\n",
        "            lower_mask, lower_label,\n",
        "            outer_mask, outer_label\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzdClY72eQHN"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IreI3UKeSBb",
        "outputId": "ae0f1923-c945-42de-8cbf-5d36fe89c19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 12701\n",
            "Train samples: 10160\n",
            "Val samples: 2541\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "dataset = DeepFashionMultiFabricDataset(\n",
        "    img_dir=IMAGE_DIR,\n",
        "    segm_dir=SEGM_DIR,\n",
        "    fabric_ann=fabric_ann,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(\n",
        "    dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Val samples:\", len(val_dataset))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmEZZzIBF1Tm"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2AvNox2eUlh"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "\n",
        "(\n",
        "    images,\n",
        "    upper_masks, upper_labels,\n",
        "    lower_masks, lower_labels,\n",
        "    outer_masks, outer_labels\n",
        ") = batch\n",
        "\n",
        "print(images.shape)        # (B, 3, 224, 224)\n",
        "print(upper_masks.shape)   # (B, 1, 224, 224)\n",
        "print(lower_masks.shape)\n",
        "print(outer_masks.shape)\n",
        "\n",
        "print(\"Upper labels:\", upper_labels[:8])\n",
        "print(\"Lower labels:\", lower_labels[:8])\n",
        "print(\"Outer labels:\", outer_labels[:8])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12a2839e"
      },
      "outputs": [],
      "source": [
        "class MultiFabricResNet(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(\n",
        "            weights=models.ResNet50_Weights.DEFAULT\n",
        "        )\n",
        "        num_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.upper_head = nn.Linear(num_features, num_classes)\n",
        "        self.lower_head = nn.Linear(num_features, num_classes)\n",
        "        self.outer_head = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        return (\n",
        "            self.upper_head(feats),\n",
        "            self.lower_head(feats),\n",
        "            self.outer_head(feats)\n",
        "        )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSmo-rF7clmP"
      },
      "outputs": [],
      "source": [
        "model = MultiFabricResNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=7)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "normalizer = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX5DduKncpjB"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "print(\"Starting multi-fabric training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        (\n",
        "            images,\n",
        "            upper_masks, upper_labels,\n",
        "            lower_masks, lower_labels,\n",
        "            outer_masks, outer_labels\n",
        "        ) = batch\n",
        "\n",
        "        images = images.to(device)\n",
        "        upper_masks = upper_masks.to(device)\n",
        "        lower_masks = lower_masks.to(device)\n",
        "        outer_masks = outer_masks.to(device)\n",
        "\n",
        "        upper_labels = upper_labels.to(device)\n",
        "        lower_labels = lower_labels.to(device)\n",
        "        outer_labels = outer_labels.to(device)\n",
        "\n",
        "        # ---- APPLY MASKS ----\n",
        "        upper_imgs = normalizer(images * upper_masks.repeat(1, 3, 1, 1))\n",
        "        lower_imgs = normalizer(images * lower_masks.repeat(1, 3, 1, 1))\n",
        "        outer_imgs = normalizer(images * outer_masks.repeat(1, 3, 1, 1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ---- FORWARD ----\n",
        "        up_out, _, _ = model(upper_imgs)\n",
        "        _, low_out, _ = model(lower_imgs)\n",
        "        _, _, out_out = model(outer_imgs)\n",
        "\n",
        "        # ---- SAFE LOSSES ----\n",
        "        loss_upper = safe_ce_loss(up_out, upper_labels, criterion)\n",
        "        loss_lower = safe_ce_loss(low_out, lower_labels, criterion)\n",
        "        loss_outer = safe_ce_loss(out_out, outer_labels, criterion)\n",
        "\n",
        "        loss = 0.0\n",
        "        count = 0\n",
        "        for l in [loss_upper, loss_lower, loss_outer]:\n",
        "            if l is not None:\n",
        "                loss += l\n",
        "                count += 1\n",
        "\n",
        "        # Skip batch if *all* heads were invalid (very rare, but safe)\n",
        "        if count == 0:\n",
        "            continue\n",
        "\n",
        "        # Average over valid heads\n",
        "        loss = loss / count\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "                f\"Step [{i}] \"\n",
        "                f\"Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    avg_loss = running_loss / max(num_batches, 1)\n",
        "    print(f\"Epoch {epoch+1} finished. Avg loss: {avg_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrXyVzebGPxK"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "correct = {\"upper\": 0, \"lower\": 0, \"outer\": 0}\n",
        "total   = {\"upper\": 0, \"lower\": 0, \"outer\": 0}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "\n",
        "        (\n",
        "            images,\n",
        "            upper_masks, upper_labels,\n",
        "            lower_masks, lower_labels,\n",
        "            outer_masks, outer_labels\n",
        "        ) = batch\n",
        "\n",
        "        images = images.to(device)\n",
        "        upper_masks = upper_masks.to(device)\n",
        "        lower_masks = lower_masks.to(device)\n",
        "        outer_masks = outer_masks.to(device)\n",
        "\n",
        "        upper_labels = upper_labels.to(device)\n",
        "        lower_labels = lower_labels.to(device)\n",
        "        outer_labels = outer_labels.to(device)\n",
        "\n",
        "        # Apply masks\n",
        "        upper_imgs = normalizer(images * upper_masks.repeat(1,3,1,1))\n",
        "        lower_imgs = normalizer(images * lower_masks.repeat(1,3,1,1))\n",
        "        outer_imgs = normalizer(images * outer_masks.repeat(1,3,1,1))\n",
        "\n",
        "        up_out, _, _ = model(upper_imgs)\n",
        "        _, low_out, _ = model(lower_imgs)\n",
        "        _, _, out_out = model(outer_imgs)\n",
        "\n",
        "        # ---- Upper ----\n",
        "        valid = upper_labels != 7\n",
        "        correct[\"upper\"] += (up_out.argmax(1)[valid] == upper_labels[valid]).sum().item()\n",
        "        total[\"upper\"]   += valid.sum().item()\n",
        "\n",
        "        # ---- Lower ----\n",
        "        valid = lower_labels != 7\n",
        "        correct[\"lower\"] += (low_out.argmax(1)[valid] == lower_labels[valid]).sum().item()\n",
        "        total[\"lower\"]   += valid.sum().item()\n",
        "\n",
        "        # ---- Outer ----\n",
        "        valid = outer_labels != 7\n",
        "        correct[\"outer\"] += (out_out.argmax(1)[valid] == outer_labels[valid]).sum().item()\n",
        "        total[\"outer\"]   += valid.sum().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNaeCscWGSre"
      },
      "outputs": [],
      "source": [
        "for k in correct:\n",
        "    acc = correct[k] / max(total[k], 1)\n",
        "    print(f\"{k.capitalize()} garment fabric accuracy: {acc:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
